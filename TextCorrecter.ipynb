{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import nltk\n",
    "\n",
    "from correct_text import train, decode, decode_sentence, evaluate_accuracy, create_model,\\\n",
    "    DefaultPTBConfig, DefaultMovieDialogConfig\n",
    "from text_correcter_data_readers import PTBDataReader, MovieDialogReader\n",
    "from text_correcter_models import InputBiasedLanguageModel\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_data_path = \"/Users/atpaino/data/textcorrecter/dialog_corpus\"\n",
    "train_path = os.path.join(root_data_path, \"cleaned_dialog_train.txt\")\n",
    "val_path = os.path.join(root_data_path, \"cleaned_dialog_val.txt\")\n",
    "test_path = os.path.join(root_data_path, \"cleaned_dialog_test.txt\")\n",
    "model_path = os.path.join(root_data_path, \"dialog_correcter_model\")\n",
    "config = DefaultMovieDialogConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data; train = /Users/atpaino/data/textcorrecter/dialog_corpus/cleaned_dialog_train.txt, test = /Users/atpaino/data/textcorrecter/dialog_corpus/cleaned_dialog_val.txt\n",
      "Creating 2 layers of 512 units.\n",
      "Reading model parameters from /Users/atpaino/data/textcorrecter/dialog_corpus/dialog_correcter_model/translate.ckpt-15000\n",
      "Training bucket sizes: [226666, 98064, 56724, 80504]\n",
      "Total train size: 461958.0\n",
      "global step 15100 learning rate 0.4049 step-time 4.43 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.12\n",
      "  eval: bucket 3 perplexity 1.67\n",
      "global step 15200 learning rate 0.4049 step-time 5.28 perplexity 1.12\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.13\n",
      "  eval: bucket 2 perplexity 1.15\n",
      "  eval: bucket 3 perplexity 1.16\n",
      "global step 15300 learning rate 0.4049 step-time 4.65 perplexity 1.06\n",
      "  eval: bucket 0 perplexity 1.07\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 15400 learning rate 0.4049 step-time 4.84 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.08\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 15500 learning rate 0.4049 step-time 5.04 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.11\n",
      "global step 15600 learning rate 0.4049 step-time 5.26 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.10\n",
      "global step 15700 learning rate 0.4049 step-time 3.95 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 15800 learning rate 0.4049 step-time 4.10 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 15900 learning rate 0.4049 step-time 4.89 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.10\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 16000 learning rate 0.4049 step-time 3.45 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 16100 learning rate 0.4049 step-time 4.40 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 16200 learning rate 0.4008 step-time 3.99 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 16300 learning rate 0.4008 step-time 4.17 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 16400 learning rate 0.4008 step-time 4.15 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 16500 learning rate 0.3968 step-time 4.34 perplexity 1.24\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.17\n",
      "  eval: bucket 2 perplexity 1.27\n",
      "  eval: bucket 3 perplexity 3.94\n",
      "global step 16600 learning rate 0.3928 step-time 4.46 perplexity 1.11\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.13\n",
      "global step 16700 learning rate 0.3928 step-time 3.96 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.10\n",
      "global step 16800 learning rate 0.3928 step-time 4.99 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.09\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 16900 learning rate 0.3928 step-time 5.05 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 17000 learning rate 0.3928 step-time 4.13 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.09\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 17100 learning rate 0.3928 step-time 4.18 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 17200 learning rate 0.3928 step-time 4.32 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 17300 learning rate 0.3928 step-time 4.70 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.12\n",
      "  eval: bucket 3 perplexity 1.90\n",
      "global step 17400 learning rate 0.3889 step-time 4.17 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.10\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 17500 learning rate 0.3850 step-time 4.86 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.07\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 17600 learning rate 0.3850 step-time 4.35 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 17700 learning rate 0.3850 step-time 4.67 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 17800 learning rate 0.3850 step-time 4.88 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.08\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 17900 learning rate 0.3850 step-time 3.76 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 18000 learning rate 0.3850 step-time 4.77 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 18100 learning rate 0.3850 step-time 4.17 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 18200 learning rate 0.3850 step-time 4.38 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 18300 learning rate 0.3850 step-time 4.79 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 18400 learning rate 0.3850 step-time 4.57 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 18500 learning rate 0.3812 step-time 4.42 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 18600 learning rate 0.3812 step-time 4.36 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.29\n",
      "global step 18700 learning rate 0.3812 step-time 4.30 perplexity 1.06\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.10\n",
      "global step 18800 learning rate 0.3774 step-time 4.54 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.11\n",
      "global step 18900 learning rate 0.3774 step-time 4.43 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19000 learning rate 0.3774 step-time 4.48 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19100 learning rate 0.3774 step-time 4.63 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19200 learning rate 0.3774 step-time 4.42 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19300 learning rate 0.3774 step-time 4.86 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 19400 learning rate 0.3774 step-time 4.21 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 19500 learning rate 0.3774 step-time 4.68 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19600 learning rate 0.3736 step-time 4.53 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19700 learning rate 0.3736 step-time 4.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19800 learning rate 0.3736 step-time 4.47 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19900 learning rate 0.3736 step-time 4.47 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 20000 learning rate 0.3699 step-time 4.98 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 20100 learning rate 0.3662 step-time 4.56 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 20200 learning rate 0.3662 step-time 4.34 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 20300 learning rate 0.3662 step-time 4.53 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 20400 learning rate 0.3662 step-time 4.14 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 20500 learning rate 0.3662 step-time 4.47 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 20600 learning rate 0.3662 step-time 4.70 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 20700 learning rate 0.3625 step-time 4.23 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 20800 learning rate 0.3625 step-time 4.91 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 20900 learning rate 0.3625 step-time 4.00 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21000 learning rate 0.3625 step-time 4.09 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21100 learning rate 0.3625 step-time 4.33 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21200 learning rate 0.3625 step-time 4.55 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 21300 learning rate 0.3589 step-time 3.99 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 21400 learning rate 0.3589 step-time 3.88 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 21500 learning rate 0.3589 step-time 4.92 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21600 learning rate 0.3553 step-time 4.94 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 21700 learning rate 0.3517 step-time 4.54 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21800 learning rate 0.3517 step-time 4.11 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 21900 learning rate 0.3517 step-time 4.41 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22000 learning rate 0.3517 step-time 4.89 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22100 learning rate 0.3482 step-time 4.86 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22200 learning rate 0.3482 step-time 4.65 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22300 learning rate 0.3482 step-time 4.88 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 22400 learning rate 0.3482 step-time 4.05 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 22500 learning rate 0.3447 step-time 4.91 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.00\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 22600 learning rate 0.3413 step-time 4.51 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 22700 learning rate 0.3413 step-time 4.40 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 22800 learning rate 0.3413 step-time 5.01 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22900 learning rate 0.3413 step-time 4.30 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23000 learning rate 0.3413 step-time 5.05 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 23100 learning rate 0.3379 step-time 4.77 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23200 learning rate 0.3345 step-time 4.96 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23300 learning rate 0.3311 step-time 4.40 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 23400 learning rate 0.3311 step-time 4.99 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 23500 learning rate 0.3311 step-time 4.76 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23600 learning rate 0.3311 step-time 4.01 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.00\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 23700 learning rate 0.3311 step-time 5.20 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 23800 learning rate 0.3278 step-time 4.94 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 23900 learning rate 0.3278 step-time 4.35 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 24000 learning rate 0.3278 step-time 4.19 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.08\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 24100 learning rate 0.3278 step-time 4.22 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 24200 learning rate 0.3278 step-time 4.77 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 24300 learning rate 0.3278 step-time 4.72 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 24400 learning rate 0.3278 step-time 4.39 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 24500 learning rate 0.3246 step-time 4.21 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 24600 learning rate 0.3246 step-time 4.75 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 24700 learning rate 0.3213 step-time 4.60 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 24800 learning rate 0.3213 step-time 4.87 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 24900 learning rate 0.3213 step-time 4.09 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.08\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.03\n",
      "global step 25000 learning rate 0.3213 step-time 4.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25100 learning rate 0.3213 step-time 4.76 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 25200 learning rate 0.3181 step-time 4.70 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 25300 learning rate 0.3181 step-time 3.98 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.09\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 25400 learning rate 0.3181 step-time 4.81 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25500 learning rate 0.3149 step-time 4.44 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25600 learning rate 0.3149 step-time 4.19 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 25700 learning rate 0.3149 step-time 4.30 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25800 learning rate 0.3149 step-time 4.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.00\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 25900 learning rate 0.3149 step-time 4.67 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26000 learning rate 0.3149 step-time 4.30 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.03\n",
      "global step 26100 learning rate 0.3149 step-time 4.62 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26200 learning rate 0.3149 step-time 4.79 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 26300 learning rate 0.3118 step-time 5.10 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26400 learning rate 0.3086 step-time 5.24 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26500 learning rate 0.3056 step-time 4.36 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 26600 learning rate 0.3056 step-time 5.01 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 26700 learning rate 0.3056 step-time 4.78 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 26800 learning rate 0.3056 step-time 4.74 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 26900 learning rate 0.3056 step-time 4.01 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 27000 learning rate 0.3056 step-time 5.04 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 27100 learning rate 0.3025 step-time 4.89 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 27200 learning rate 0.3025 step-time 5.43 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 27300 learning rate 0.2995 step-time 5.54 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 27400 learning rate 0.2995 step-time 4.87 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 27500 learning rate 0.2965 step-time 4.63 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 27600 learning rate 0.2965 step-time 4.44 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 27700 learning rate 0.2965 step-time 4.45 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 27800 learning rate 0.2965 step-time 4.09 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 27900 learning rate 0.2965 step-time 3.88 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28000 learning rate 0.2965 step-time 4.56 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28100 learning rate 0.2965 step-time 3.83 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.11\n",
      "global step 28200 learning rate 0.2965 step-time 4.82 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 28300 learning rate 0.2935 step-time 4.37 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 28400 learning rate 0.2935 step-time 4.55 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 28500 learning rate 0.2906 step-time 3.96 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28600 learning rate 0.2906 step-time 4.37 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28700 learning rate 0.2906 step-time 3.33 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 28800 learning rate 0.2906 step-time 4.32 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 28900 learning rate 0.2877 step-time 5.05 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 29000 learning rate 0.2848 step-time 4.44 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29100 learning rate 0.2820 step-time 4.17 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29200 learning rate 0.2820 step-time 5.45 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29300 learning rate 0.2791 step-time 4.52 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29400 learning rate 0.2791 step-time 4.38 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 29500 learning rate 0.2791 step-time 4.19 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 29600 learning rate 0.2791 step-time 3.82 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 29700 learning rate 0.2791 step-time 4.26 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 29800 learning rate 0.2791 step-time 4.11 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29900 learning rate 0.2763 step-time 4.76 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30000 learning rate 0.2736 step-time 4.08 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30100 learning rate 0.2736 step-time 4.48 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30200 learning rate 0.2708 step-time 4.07 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30300 learning rate 0.2708 step-time 5.29 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30400 learning rate 0.2708 step-time 4.75 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 30500 learning rate 0.2708 step-time 5.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30600 learning rate 0.2681 step-time 5.95 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 30700 learning rate 0.2655 step-time 4.22 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30800 learning rate 0.2655 step-time 5.12 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30900 learning rate 0.2655 step-time 4.57 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 31000 learning rate 0.2655 step-time 4.46 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 31100 learning rate 0.2655 step-time 4.41 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 31200 learning rate 0.2655 step-time 4.72 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 31300 learning rate 0.2655 step-time 3.63 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c4f4a34e9f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/atpaino/github/deep-text-correcter/correct_text.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_reader, train_path, test_path, model_path)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 train_data, bucket_id)\n\u001b[1;32m    181\u001b[0m             _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n\u001b[0;32m--> 182\u001b[0;31m                                          target_weights, bucket_id, False)\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mstep_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atpaino/github/deep-text-correcter/text_correcter_models.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(data_reader, train_path, val_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path, dropout_prob=0.25, replacement_prob=0.25, dataset_copies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram_model = InputBiasedLanguageModel(data_reader, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.800534625413185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.prob(\"hello\", [], [\"hello\", \"friend\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3200131397951014"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.prob(\"friend\", [], [\"hello\", \"friend\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.prob(\"friend\", [\"hello\"], [\"hello\", \"friend\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from /Users/atpaino/data/textcorrecter/dialog_corpus/dialog_correcter_model/translate.ckpt-31300\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "model = create_model(sess, True, model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj prob of the is 0.0032125145679, orig prob is 7.7070453699e-06\n",
      "adj prob of have is 0.213768005877, orig prob is 2.38067727309e-07\n",
      "adj prob of you is 1.81305201706, orig prob is 0.99992454052\n",
      "adj prob of a is 0.00141589109651, orig prob is 2.20189372158e-06\n",
      "adj prob of girlfriend is 0.213336618282, orig prob is 6.69496704982e-15\n",
      "adj prob of an is 0.00022046399046, orig prob is 3.24303144339e-08\n",
      "Using token you\n",
      "adj prob of 've is 0.0039547956255, orig prob is 2.80572476186e-05\n",
      "adj prob of have is 1.78443975162, orig prob is 0.979787528515\n",
      "adj prob of girlfriend is 0.213333333333, orig prob is 2.83150137722e-14\n",
      "adj prob of a is 0.00155948821844, orig prob is 2.33249593862e-07\n",
      "adj prob of 's is 8.33846878109e-05, orig prob is 7.39336147859e-09\n",
      "adj prob of 'll is 0.0233438322684, orig prob is 0.0201786737889\n",
      "Using token have\n",
      "adj prob of the is 0.0273468691474, orig prob is 0.0202672537416\n",
      "adj prob of girlfriend is 0.807891923189, orig prob is 0.00789192318916\n",
      "adj prob of a is 0.989217468643, orig prob is 0.965017914772\n",
      "adj prob of an is 0.00485434061324, orig prob is 0.00278543890454\n",
      "adj prob of 's is 0.000286309923669, orig prob is 0.000206309923669\n",
      "Using token a\n",
      "adj prob of girlfriend is 0.973558107225, orig prob is 0.760093033314\n",
      "Using token girlfriend\n",
      "Input: you have girlfriend\n",
      "Output: you have a girlfriend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test a sample from the test dataset.\n",
    "decoded = decode_sentence(sess, model, data_reader, \"you have girlfriend\", ngram_model=ngram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj prob of say is 0.0428358415407, orig prob is 1.21358530863e-11\n",
      "adj prob of they is 0.0442951800221, orig prob is 7.74678444223e-14\n",
      "adj prob of develop is 0.042666694946, orig prob is 2.82793788386e-08\n",
      "adj prob of did is 1.8010124398, orig prob is 0.999998211861\n",
      "adj prob of going is 0.0426937674941, orig prob is 5.47143812191e-14\n",
      "adj prob of n't is 0.0426666666701, orig prob is 3.4008301817e-12\n",
      "adj prob of the is 0.00320486693163, orig prob is 5.94090927564e-08\n",
      "adj prob of ... is 0.044791211328, orig prob is 4.04085032102e-09\n",
      "adj prob of thing is 0.042674067052, orig prob is 9.25055143597e-09\n",
      "adj prob of to is 0.0430280110321, orig prob is 1.41684895153e-13\n",
      "adj prob of new is 0.0427028011032, orig prob is 1.59016565682e-16\n",
      "adj prob of you is 0.0557941432159, orig prob is 5.78321938949e-12\n",
      "adj prob of 're is 0.0426666666667, orig prob is 1.62911550737e-17\n",
      "adj prob of that is 0.0466816953073, orig prob is 9.34927862125e-16\n",
      "adj prob of this is 0.0442155200207, orig prob is 6.44745411574e-12\n",
      "Using token did\n",
      "adj prob of say is 0.0428569780857, orig prob is 3.28701042079e-13\n",
      "adj prob of 've is 8.00000029184e-05, orig prob is 2.91839230733e-12\n",
      "adj prob of they is 0.0451753172448, orig prob is 5.90549137369e-11\n",
      "adj prob of develop is 0.0426666666667, orig prob is 3.35285930421e-14\n",
      "adj prob of going is 0.0426666666667, orig prob is 1.37294908676e-17\n",
      "adj prob of n't is 1.88072646478, orig prob is 0.999999821186\n",
      "adj prob of ... is 0.0437393310265, orig prob is 2.20584419386e-18\n",
      "adj prob of thing is 0.0426666666667, orig prob is 5.92135625909e-20\n",
      "adj prob of to is 0.0437912341407, orig prob is 6.72656286106e-16\n",
      "adj prob of new is 0.0426666666667, orig prob is 4.4515288353e-23\n",
      "adj prob of you is 0.0873034918336, orig prob is 1.46966243619e-07\n",
      "adj prob of 're is 0.0426666666667, orig prob is 1.02126605286e-17\n",
      "adj prob of that is 0.0441891580162, orig prob is 1.26607098828e-14\n",
      "adj prob of this is 0.0447081891583, orig prob is 2.37622204899e-13\n",
      "Using token n't\n",
      "adj prob of say is 0.0447451528248, orig prob is 9.55451810114e-07\n",
      "adj prob of 've is 0.000121096410729, orig prob is 2.39675723712e-09\n",
      "adj prob of they is 0.0434476437672, orig prob is 1.90835024227e-07\n",
      "adj prob of develop is 0.0426712338887, orig prob is 1.22052190576e-09\n",
      "adj prob of going is 0.0431278328235, orig prob is 1.5603827502e-14\n",
      "adj prob of the is 0.00092929797328, orig prob is 2.16845261747e-08\n",
      "adj prob of ... is 0.0438857890814, orig prob is 2.43118486497e-13\n",
      "adj prob of thing is 0.0426712326682, orig prob is 2.51942257381e-19\n",
      "adj prob of to is 0.0427305907014, orig prob is 1.30167213813e-11\n",
      "adj prob of new is 0.0426712326682, orig prob is 5.55470773396e-17\n",
      "adj prob of you is 1.81313952196, orig prob is 0.999998569489\n",
      "adj prob of a is 0.00127172919449, orig prob is 2.78930611941e-09\n",
      "adj prob of an is 0.000166756039865, orig prob is 2.01036876035e-09\n",
      "adj prob of 're is 0.0426666666667, orig prob is 1.28575264585e-14\n",
      "adj prob of that is 0.0440684291456, orig prob is 2.33866983938e-12\n",
      "adj prob of this is 0.0429771547722, orig prob is 2.20045302767e-16\n",
      "Using token you\n",
      "adj prob of say is 1.80113624746, orig prob is 0.999849498272\n",
      "adj prob of 've is 0.00396438945765, orig prob is 3.76510797651e-05\n",
      "adj prob of they is 0.0427021282599, orig prob is 1.50445530599e-12\n",
      "adj prob of develop is 0.0426702709526, orig prob is 1.91563867702e-06\n",
      "adj prob of going is 0.0436950554814, orig prob is 2.6548365728e-09\n",
      "adj prob of the is 0.000687860973243, orig prob is 1.63661957231e-06\n",
      "adj prob of ... is 0.0441661854022, orig prob is 2.45704606834e-14\n",
      "adj prob of thing is 0.0426683553139, orig prob is 4.50987615579e-14\n",
      "adj prob of to is 0.0459612174021, orig prob is 4.432343842e-14\n",
      "adj prob of new is 0.042673421302, orig prob is 4.6440309931e-11\n",
      "adj prob of a is 0.00156165781428, orig prob is 2.40284543906e-06\n",
      "adj prob of an is 0.000217171502434, orig prob is 7.14566613169e-06\n",
      "adj prob of 're is 0.0611354013728, orig prob is 9.46279860026e-12\n",
      "adj prob of 'll is 0.00317689208169, orig prob is 1.17336021503e-05\n",
      "adj prob of this is 0.04295035941, orig prob is 9.56041582706e-12\n",
      "adj prob of that is 0.0432306749601, orig prob is 1.20352630639e-10\n",
      "Using token say\n",
      "adj prob of they is 0.0440316321095, orig prob is 1.74766719283e-05\n",
      "adj prob of develop is 0.0426666682282, orig prob is 1.56151369612e-09\n",
      "adj prob of there is 0.000653151660255, orig prob is 1.48975743741e-06\n",
      "adj prob of the is 0.00400330812456, orig prob is 3.3407909541e-06\n",
      "adj prob of ... is 0.0465866340003, orig prob is 6.69295788767e-15\n",
      "adj prob of thing is 0.0426666666667, orig prob is 8.21672103187e-14\n",
      "adj prob of to is 0.0472399618969, orig prob is 7.66909973027e-12\n",
      "adj prob of new is 0.0426666666837, orig prob is 1.70440675462e-11\n",
      "adj prob of going is 0.0426666666838, orig prob is 1.71741353777e-11\n",
      "adj prob of 're is 0.0426666666669, orig prob is 1.86644374001e-13\n",
      "adj prob of 's is 8.02007411126e-05, orig prob is 2.00741112621e-07\n",
      "adj prob of that is 1.81789517781, orig prob is 0.999846994877\n",
      "adj prob of this is 0.0451169479131, orig prob is 3.01662936408e-07\n",
      "Using token that\n",
      "adj prob of 've is 9.62491073639e-05, orig prob is 5.60865558441e-08\n",
      "adj prob of they is 1.80067945505, orig prob is 0.999853610992\n",
      "adj prob of develop is 0.0426720643706, orig prob is 3.03588161554e-11\n",
      "adj prob of going is 0.0427044503819, orig prob is 3.41118484947e-16\n",
      "adj prob of the is 0.0022984743244, orig prob is 3.04737035606e-08\n",
      "adj prob of ... is 0.0445666477748, orig prob is 2.71670181604e-14\n",
      "adj prob of thing is 0.0435842711791, orig prob is 1.61292874084e-17\n",
      "adj prob of to is 0.0435141014223, orig prob is 3.6557260989e-14\n",
      "adj prob of new is 0.0427800178123, orig prob is 1.73212213192e-15\n",
      "adj prob of a is 0.00113255068744, orig prob is 4.33491553764e-09\n",
      "adj prob of 're is 0.0426774631201, orig prob is 1.10621922733e-09\n",
      "adj prob of 's is 0.0476983793507, orig prob is 1.02827861781e-07\n",
      "adj prob of 'll is 0.000830284504469, orig prob is 7.87369724975e-09\n",
      "adj prob of this is 0.043282001458, orig prob is 6.57543572542e-13\n",
      "Using token they\n",
      "adj prob of 've is 0.00524701287297, orig prob is 6.31573186638e-07\n",
      "adj prob of develop is 0.042666666673, orig prob is 6.34231859739e-12\n",
      "adj prob of going is 0.0429049112045, orig prob is 2.52272212009e-08\n",
      "adj prob of ... is 0.0431877714089, orig prob is 1.37995680421e-13\n",
      "adj prob of thing is 0.0426666666667, orig prob is 4.97728768992e-17\n",
      "adj prob of to is 0.0426964441094, orig prob is 2.88933061504e-11\n",
      "adj prob of new is 0.0426666666667, orig prob is 2.14412160444e-14\n",
      "adj prob of 're is 1.83673108316, orig prob is 0.99998575449\n",
      "adj prob of 's is 0.000303665802983, orig prob is 3.35199246138e-07\n",
      "adj prob of 'll is 0.00962490573378, orig prob is 1.24460075313e-06\n",
      "adj prob of this is 0.0426666666667, orig prob is 2.16761742479e-20\n",
      "Using token 're\n",
      "adj prob of develop is 0.0426717995419, orig prob is 5.13287523063e-06\n",
      "adj prob of ... is 0.0433303961553, orig prob is 1.55931361573e-11\n",
      "adj prob of going is 1.81171663887, orig prob is 0.999758064747\n",
      "adj prob of thing is 0.0426666666672, orig prob is 5.60488775617e-13\n",
      "adj prob of to is 0.0428840956583, orig prob is 3.71179920133e-10\n",
      "adj prob of new is 0.0427468155941, orig prob is 4.36462279652e-08\n",
      "adj prob of a is 0.0106546284718, orig prob is 7.3134987133e-07\n",
      "adj prob of an is 0.00106920933433, orig prob is 5.05873640577e-06\n",
      "adj prob of 's is 8.45589085858e-05, orig prob is 4.55890858575e-06\n",
      "adj prob of this is 0.0427009976246, orig prob is 1.2314381459e-10\n",
      "Using token going\n",
      "adj prob of develop is 0.0426666842668, orig prob is 1.76001382357e-08\n",
      "adj prob of the is 0.000195332473993, orig prob is 1.41689255884e-07\n",
      "adj prob of ... is 0.0436458850247, orig prob is 9.6687742257e-08\n",
      "adj prob of to is 1.92576290306, orig prob is 0.99991697073\n",
      "adj prob of new is 0.0426666666668, orig prob is 1.21600822402e-13\n",
      "adj prob of thing is 0.0426666666698, orig prob is 3.16193946026e-12\n",
      "adj prob of a is 0.000167125950459, orig prob is 7.32861906272e-07\n",
      "adj prob of 's is 0.000110896731914, orig prob is 3.08967319143e-05\n",
      "adj prob of this is 0.042753059775, orig prob is 1.97345490643e-11\n",
      "Using token to\n",
      "adj prob of 've is 9.43804912772e-05, orig prob is 4.98920326208e-06\n",
      "adj prob of develop is 1.79583614821, orig prob is 0.995811104774\n",
      "adj prob of the is 0.0120103919761, orig prob is 0.00166884460486\n",
      "adj prob of ... is 0.0436121657504, orig prob is 1.09423524464e-07\n",
      "adj prob of thing is 0.0426666707018, orig prob is 4.03512512293e-09\n",
      "adj prob of new is 0.0429578032676, orig prob is 6.67245236841e-09\n",
      "adj prob of a is 0.00383915523777, orig prob is 0.00144263752736\n",
      "adj prob of an is 0.000561500410071, orig prob is 0.000271761644399\n",
      "adj prob of 's is 8.21535358883e-05, orig prob is 2.15353588828e-06\n",
      "adj prob of 'll is 8.0773960096e-05, orig prob is 7.73960096012e-07\n",
      "adj prob of this is 0.0442633741659, orig prob is 0.00075149157783\n",
      "Using token develop\n",
      "adj prob of revolutionary is 0.0426706426742, orig prob is 3.97600751967e-06\n",
      "adj prob of the is 0.0154678425149, orig prob is 3.22713026435e-06\n",
      "adj prob of ... is 0.050358974359, orig prob is 5.75238776968e-14\n",
      "adj prob of thing is 0.0426666666684, orig prob is 1.77329212512e-12\n",
      "adj prob of 'm is 8.00447170088e-05, orig prob is 4.47170087625e-08\n",
      "adj prob of new is 0.0426666667148, orig prob is 4.81742840064e-11\n",
      "adj prob of a is 0.0462513819611, orig prob is 1.75358072738e-05\n",
      "adj prob of an is 0.00777253326982, orig prob is 2.25577508672e-07\n",
      "adj prob of their is 8.01064500523e-05, orig prob is 1.06450052328e-07\n",
      "adj prob of 's is 8.40458521653e-05, orig prob is 4.04585216529e-06\n",
      "adj prob of this is 1.79994438887, orig prob is 0.999944388866\n",
      "Using token this\n",
      "adj prob of 've is 8.04886065881e-05, orig prob is 4.88606588078e-07\n",
      "adj prob of revolutionary is 1.79937605858, orig prob is 0.999376058578\n",
      "adj prob of the is 0.000791511205757, orig prob is 7.86644159234e-05\n",
      "adj prob of ... is 0.045414820155, orig prob is 2.4149969704e-06\n",
      "adj prob of 'm is 8.2838398359e-05, orig prob is 2.83839835902e-06\n",
      "adj prob of new is 0.0429014840519, orig prob is 5.16405940232e-08\n",
      "adj prob of thing is 0.0454330085223, orig prob is 1.88951730706e-07\n",
      "adj prob of a is 0.00144957153157, orig prob is 0.000471337378258\n",
      "adj prob of an is 0.000171581128802, orig prob is 2.01306847885e-05\n",
      "adj prob of 's is 0.000137484114223, orig prob is 3.70697016479e-05\n",
      "adj prob of 'll is 0.000448532721394, orig prob is 1.07329503862e-06\n",
      "Using token revolutionary\n",
      "adj prob of the is 0.0592788861561, orig prob is 0.0591988861561\n",
      "adj prob of ... is 0.0426666666959, orig prob is 2.92004892488e-11\n",
      "adj prob of thing is 0.0426670019861, orig prob is 3.35319384703e-07\n",
      "adj prob of new is 1.13279040456, orig prob is 0.332790404558\n",
      "adj prob of an is 0.000664054447245, orig prob is 0.000584054447245\n",
      "adj prob of their is 0.000219758878504, orig prob is 0.000139758878504\n",
      "adj prob of a is 0.3471690522, orig prob is 0.3470890522\n",
      "adj prob of 's is 0.258787731962, orig prob is 0.258707731962\n",
      "Using token new\n",
      "adj prob of ... is 0.043252833138, orig prob is 3.47847462911e-14\n",
      "adj prob of thing is 0.800589741677, orig prob is 3.57520571015e-06\n",
      "Using token thing\n",
      "adj prob of ... is 0.805127371267, orig prob is 7.12642247436e-07\n",
      "adj prob of a is 0.000864213205125, orig prob is 1.36003677653e-07\n",
      "adj prob of 's is 0.00250072185816, orig prob is 8.17662294139e-06\n",
      "Using token ...\n",
      "adj prob of the is 0.00394145377797, orig prob is 1.07844201125e-07\n",
      "adj prob of a is 0.00236191385344, orig prob is 5.45032150967e-07\n",
      "adj prob of 's is 0.000167895026793, orig prob is 6.57458149362e-05\n",
      "adj prob of 'll is 8.05696943504e-05, orig prob is 5.69694350361e-07\n",
      "Using token the\n",
      "Input: did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "Output: did n't you say that they 're going to develop this revolutionary new thing ... the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded = decode_sentence(sess, model, data_reader,\n",
    "                          \"did n't you say that they 're going to develop this revolutionary new thing ...\",\n",
    "                          ngram_model=ngram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kvothe', 'went', 'to', 'the', 'market']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"kvothe went to market\", ngram_model=ngram_model, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blablahblah', 'and', 'bladdddd', 'went', 'to', 'market', 'market']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"blablahblah and bladdddd went to market\", ngram_model=ngram_model, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: do you have book\n",
      "Output: do you have a book\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"do you have book\", ngram_model=ngram_model, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'did', 'better', 'then', 'him']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"she did better then him\", ngram_model=ngram_model, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8136\n",
      "\tModel BLEU = 0.8238\n",
      "\tBaseline Accuracy: 0.8891\n",
      "\tModel Accuracy: 0.9238\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8855\n",
      "\tModel BLEU = 0.8797\n",
      "\tBaseline Accuracy: 0.7927\n",
      "\tModel Accuracy: 0.8598\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.9057\n",
      "\tModel BLEU = 0.8814\n",
      "\tBaseline Accuracy: 0.8091\n",
      "\tModel Accuracy: 0.8000\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9018\n",
      "\tModel BLEU = 0.9030\n",
      "\tBaseline Accuracy: 0.6423\n",
      "\tModel Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "errors = evaluate_accuracy(sess, model, data_reader, ngram_model, test_path, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: you 'll beg for mercy in second second .\n",
      "Target:   you 'll beg for mercy in a second .\n",
      "\n",
      "Decoding: i 'm dying for a shower . you could use one one too . and we 'd better check that bandage .\n",
      "Target:   i 'm dying for a shower . you could use one too . and we 'd better check that bandage .\n",
      "\n",
      "Decoding: listen . understand . i 'm not a military objective , reese . i 'm person person ... you do n't own me .\n",
      "Target:   listen . understand . i 'm not a military objective , reese . i 'm a person ... you do n't own me .\n",
      "\n",
      "Decoding: whatever ... they become the hotshot computer guys so they get the job to build el computer grande ... skynet ... for government government . right ?\n",
      "Target:   whatever ... they become the hotshot computer guys so they get the job to build el computer grande ... skynet ... for the government . right ?\n",
      "\n",
      "Decoding: did n't you say that they 're going to develop this revolutionary new new thing ...\n",
      "Target:   did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "\n",
      "Decoding: sleep . it 'll be light light soon .\n",
      "Target:   sleep . it 'll be light soon .\n",
      "\n",
      "Decoding: well , at least i know what to name him . i do n't suppose you 'd know who father is ? so i do n't tell him to get lost when i meet him .\n",
      "Target:   well , at least i know what to name him . i do n't suppose you 'd know who the father is ? so i do n't tell him to get lost when i meet him .\n",
      "\n",
      "Decoding: hunter killers . patrol machines . build build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "Target:   hunter killers . patrol machines . build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "\n",
      "Decoding: did n't see the war . i was born after , in ruins . grew up there . starving . hiding from the h-k .\n",
      "Target:   did n't see the war . i was born after , in the ruins . grew up there . starving . hiding from the h-k 's .\n",
      "\n",
      "Decoding: -- everythingis gone . just gone . there were survivors . here . there . nobody knew who started it . it was machines .\n",
      "Target:   -- everythingis gone . just gone . there were survivors . here . there . nobody knew who started it . it was the machines .\n",
      "\n",
      "Decoding: but outside , it 's a living human tissue . flesh , skin , hair ... blood . grown for cyborgs .\n",
      "Target:   but outside , it 's living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "\n",
      "Decoding: most official records were lost in the war . the computer knew almost nothing about connor mother . her name . where she she lived , just the city . no scanner pictures . terminator was just being systematic .\n",
      "Target:   most official records were lost in the war . the computer knew almost nothing about connor 's mother . her name . where she lived , just the city . no scanner pictures . the terminator was just being systematic .\n",
      "\n",
      "Decoding: why were other two women killed ?\n",
      "Target:   why were the other two women killed ?\n",
      "\n",
      "Decoding: ca n't . nobody goes home . nobody else comes through . it just him and me .\n",
      "Target:   ca n't . nobody goes home . nobody else comes through . it 's just him and me .\n",
      "\n",
      "Decoding: year year 2027 ?\n",
      "Target:   the year 2027 ?\n",
      "\n",
      "Decoding: why do n't you just stretch out here and get some sleep . it 'll take your mom a a good hour to get here from redlands .\n",
      "Target:   why do n't you just stretch out here and get some sleep . it 'll take your mom a good hour to get here from redlands .\n",
      "\n",
      "Decoding: they already already been identi- fied . there 's no doubt .\n",
      "Target:   they 've already been identi- fied . there 's no doubt .\n",
      "\n",
      "Decoding: if they can get this on tube by eleven , she may just call us . how do i look ?\n",
      "Target:   if they can get this on the tube by eleven , she may just call us . how do i look ?\n",
      "\n",
      "Decoding: to make a statement . i 'm gon na give them name . maybe the jackals can help us out for once .\n",
      "Target:   to make a statement . i 'm gon na give them the name . maybe the jackals can help us out for once .\n",
      "\n",
      "Decoding: i hate the press cases . especially weird press cases . where you going ?\n",
      "Target:   i hate the press cases . especially the weird press cases . where you going ?\n",
      "\n",
      "Decoding: i already did . no answer at the door and the apartment manager 's out . i keeping them there .\n",
      "Target:   i already did . no answer at the door and the apartment manager 's out . i 'm keeping them there .\n",
      "\n",
      "Decoding: i put put cigarette cigarette out in it .\n",
      "Target:   i put a cigarette out in it .\n",
      "\n",
      "Decoding: okay , let 's see ... got a pos- itive on her . she sarah connor , , works as a legal --\n",
      "Target:   okay , let 's see ... got a pos- itive on her . she 's sarah connor , works as a legal --\n",
      "\n",
      "Decoding: because it 's fair . give me next next quarter . if you still feel this way , vote your shares ...\n",
      "Target:   because it 's fair . give me next quarter . if you still feel this way , vote your shares ...\n",
      "\n",
      "Decoding: it probably probably will . in fact , i 'd go so far as to say it 's almost certainly will , in time . why should i settle for that ?\n",
      "Target:   it probably will . in fact , i 'd go so far as to say it almost certainly will , in time . why should i settle for that ?\n",
      "\n",
      "Decoding: you promised you 'd meet projections , alan . a dollar sixty per share you said . so , i do n't think this is so surprising visit visit .\n",
      "Target:   you promised you 'd meet projections , alan . a dollar sixty per share you said . so , i do n't think this is so surprising a visit .\n",
      "\n",
      "Decoding: single single ?\n",
      "Target:   single ?\n",
      "\n",
      "Decoding: listen very carefully , i 'm telling the truth ... this is game game . this was all game .\n",
      "Target:   listen very carefully , i 'm telling the truth ... this is the game . this was all the game .\n",
      "\n",
      "Decoding: that 's a gun . that 's ... that 's not automatic . the guard had an automatic ...\n",
      "Target:   that gun . that ... that 's not automatic . the guard had an automatic ...\n",
      "\n",
      "Decoding: call them , tell them to get boss boss up here . tell them i 'll kill you ...\n",
      "Target:   call them , tell them to get the boss up here . tell them i 'll kill you ...\n",
      "\n",
      "Decoding: i do n't know . lot lot .\n",
      "Target:   i do n't know . a lot .\n",
      "\n",
      "Decoding: that 's lie ...\n",
      "Target:   that 's a lie ...\n",
      "\n",
      "Decoding: you do n't have choice . no one else is going to tell you what 's going on .\n",
      "Target:   you do n't have a choice . no one else is going to tell you what 's going on .\n",
      "\n",
      "Decoding: take picture picture out of the frame and show it to me .\n",
      "Target:   take the picture out of the frame and show it to me .\n",
      "\n",
      "Decoding: take a picture out .\n",
      "Target:   take the picture out .\n",
      "\n",
      "Decoding: it was left in my hotel room room , well , not really mine . you 're saying it 's not you ?\n",
      "Target:   it was left in my hotel room , well , not really mine . you 're saying it 's not you ?\n",
      "\n",
      "Decoding: they said five hundred . i said six . they said man in gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "Target:   they said five hundred . i said six . they said the man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "\n",
      "Decoding: i know owner of campton place . i could talk to him in the morning .\n",
      "Target:   i know the owner of campton place . i could talk to him in the morning .\n",
      "\n",
      "Decoding: investment banking . moving money from place place to place .\n",
      "Target:   investment banking . moving money from place to place .\n",
      "\n",
      "Decoding: nicholas van orton ? what are you , czar ?\n",
      "Target:   nicholas van orton ? what are you , a czar ?\n",
      "\n",
      "Decoding: ... two hundred dollars toe .\n",
      "Target:   ... two hundred dollars a toe .\n",
      "\n",
      "Decoding: there goes thousand dollars .\n",
      "Target:   there goes a thousand dollars .\n",
      "\n",
      "Decoding: you 're grown man . i 'm not responsible for you .\n",
      "Target:   you 're a grown man . i 'm not responsible for you .\n",
      "\n",
      "Decoding: what 's c c .r .s . ?\n",
      "Target:   what 's c .r .s . ?\n",
      "\n",
      "Decoding: this is c c .r .s .\n",
      "Target:   this is c .r .s .\n",
      "\n",
      "Decoding: i give you a boost .\n",
      "Target:   i 'll give you a boost .\n",
      "\n",
      "Decoding: read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use emergency emergency ... ``\n",
      "Target:   read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use emergency ... ``\n",
      "\n",
      "Decoding: how thoughtful . a gift of inconvenience .\n",
      "Target:   how thoughtful . the gift of inconvenience .\n",
      "\n",
      "Decoding: how do you know that way ?\n",
      "Target:   how do you know that 's the way ?\n",
      "\n",
      "Decoding: it 's run by a company ... they play play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "Target:   it 's run by a company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "\n",
      "Decoding: i was trying to tell you ... it 's game .\n",
      "Target:   i was trying to tell you ... it 's a game .\n",
      "\n",
      "Decoding: ten minutes ago , i 'm looking forward to a quiet dinner . i get note note ...\n",
      "Target:   ten minutes ago , i 'm looking forward to a quiet dinner . i get a note ...\n",
      "\n",
      "Decoding: he pissing his pants . is that real enough for you ? call 911 !\n",
      "Target:   he 's pissing his pants . is that real enough for you ? call 911 !\n",
      "\n",
      "Decoding: bad month . you did exact same same thing to me last week .\n",
      "Target:   a bad month . you did the exact same thing to me last week .\n",
      "\n",
      "Decoding: yeah , yeah . she called called a cab . said something about catching plane .\n",
      "Target:   yeah , yeah . she called a cab . said something about catching a plane .\n",
      "\n",
      "Decoding: oh , god yes please . thanks , man . i take you up on that .\n",
      "Target:   oh , god yes please . thanks , man . i 'll take you up on that .\n",
      "\n",
      "Decoding: this ... ? oh , this is just ... this is bill .\n",
      "Target:   this ... ? oh , this is just ... this is the bill .\n",
      "\n",
      "Decoding: you know ... how 'd you manage gun gun ? i mean ...\n",
      "Target:   you know ... how 'd you manage the gun ? i mean ...\n",
      "\n",
      "Decoding: it does n't stop , nick . i paid bill , i gave 'em their money , but it all started again . they wo n't leave me alone ...\n",
      "Target:   it does n't stop , nick . i paid the bill , i gave 'em their money , but it all started again . they wo n't leave me alone ...\n",
      "\n",
      "Decoding: c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i 've almost got you into .\n",
      "Target:   c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i almost got you into .\n",
      "\n",
      "Decoding: just drive , man . it fucking fucking nuts !\n",
      "Target:   just drive , man . it 's fucking nuts !\n",
      "\n",
      "Decoding: tell me you call .\n",
      "Target:   tell me you 'll call .\n",
      "\n",
      "Decoding: like stroke ?\n",
      "Target:   like a stroke ?\n",
      "\n",
      "Decoding: this is n't escort service ?\n",
      "Target:   this is n't an escort service ?\n",
      "\n",
      "Decoding: you 've heard of it . you seen other people having it . they 're an entertainment service , but more than that .\n",
      "Target:   you 've heard of it . you 've seen other people having it . they 're an entertainment service , but more than that .\n",
      "\n",
      "Decoding: they make your life fun . there only only guarantee is you will not be bored .\n",
      "Target:   they make your life fun . their only guarantee is you will not be bored .\n",
      "\n",
      "Decoding: how much is it ? a few thousand , at least . rolex like that ... lucky for you they missed it .\n",
      "Target:   how much is it ? a few thousand , at least . a rolex like that ... lucky for you they missed it .\n",
      "\n",
      "Decoding: well , where to begin . it complicated ...\n",
      "Target:   well , where to begin . it 's complicated ...\n",
      "\n",
      "Decoding: no money , no identification or passport . you are in fix . what did happen to you ?\n",
      "Target:   no money , no identification or passport . you are in a fix . what did happen to you ?\n",
      "\n",
      "Decoding: i call . i really will .\n",
      "Target:   i 'll call . i really will .\n",
      "\n",
      "Decoding: i told you , they hired me over the phone . i 've never met anyone .\n",
      "Target:   i told you , they hired me over the phone . i never met anyone .\n",
      "\n",
      "Decoding: shut up . cuff him to rail .\n",
      "Target:   shut up . cuff him to the rail .\n",
      "\n",
      "Decoding: they own whole building . they just move from floor floor to floor .\n",
      "Target:   they own the whole building . they just move from floor to floor .\n",
      "\n",
      "Decoding: i 'm tired of dealing with peons . i need to get to whoever in charge .\n",
      "Target:   i 'm tired of dealing with peons . i need to get to whoever 's in charge .\n",
      "\n",
      "Decoding: look , it was just job job . nothing personal , ya know ? i play my part , improvise a little . that 's what i 'm good at .\n",
      "Target:   look , it was just a job . nothing personal , ya know ? i play my part , improvise a little . that 's what i 'm good at .\n",
      "\n",
      "Decoding: and if nothing lacking ?\n",
      "Target:   and if nothing 's lacking ?\n",
      "\n",
      "Decoding: your brother was a client with our london branch we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "Target:   your brother was a client with our london branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "\n",
      "Decoding: nobody 's worried about your father .\n",
      "Target:   nobody worried about your father .\n",
      "\n",
      "Decoding: all the time i known known you , you 've never once asked about him .\n",
      "Target:   all the time i 've known you , you 've never once asked about him .\n",
      "\n",
      "Decoding: there 's been a break in . lock this door and stay here . do n't move muscle .\n",
      "Target:   there 's been a break in . lock this door and stay here . do n't move a muscle .\n",
      "\n",
      "Decoding: i finished for evening evening . will you be needing anything else ?\n",
      "Target:   i 've finished for the evening . will you be needing anything else ?\n",
      "\n",
      "Decoding: dinner 's in oven .\n",
      "Target:   dinner 's in the oven .\n",
      "\n",
      "Decoding: there was an incident few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "Target:   there was an incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "\n",
      "Decoding: you 're here for conrad van orton ? i 'm hotel hotel manager ...\n",
      "Target:   you 're here for conrad van orton ? i 'm the hotel manager ...\n",
      "\n",
      "Decoding: what 's trouble ?\n",
      "Target:   what 's the trouble ?\n",
      "\n",
      "Decoding: `` under bleachers '' ... by seymour butts .\n",
      "Target:   `` under the bleachers '' ... by seymour butts .\n",
      "\n",
      "Decoding: what 's the gentleman , maria ?\n",
      "Target:   what gentleman , maria ?\n",
      "\n",
      "Decoding: gentleman left a message requesting lunch , but i assured him ...\n",
      "Target:   a gentleman left a message requesting a lunch , but i assured him ...\n",
      "\n",
      "Decoding: i know who she is . take message message .\n",
      "Target:   i know who she is . take a message .\n",
      "\n",
      "Decoding: because , if you do n't know about society , you do n't have satisfaction of avoiding it .\n",
      "Target:   because , if you do n't know about society , you do n't have the satisfaction of avoiding it .\n",
      "\n",
      "Decoding: let me think ... hordes of men in tuxedos . everyone droning . ludwell trying trying to break the ice by reciting off-color limerick ...\n",
      "Target:   let me think ... hordes of men in tuxedos . everyone 's droning . ludwell 's trying to break the ice by reciting an off-color limerick ...\n",
      "\n",
      "Decoding: hinchberger wedding wedding .\n",
      "Target:   the hinchberger wedding .\n",
      "\n",
      "Decoding: fitzwilliam botanical garden annual fundraiser .\n",
      "Target:   the fitzwilliam botanical garden annual fundraiser .\n",
      "\n",
      "Decoding: find out about a company called c c .r .s . consumer recreation services .\n",
      "Target:   find out about a company called c .r .s . consumer recreation services .\n",
      "\n",
      "Decoding: someone 's playing hardball . it complicated . can i ask a favor ?\n",
      "Target:   someone 's playing hardball . it 's complicated . can i ask a favor ?\n",
      "\n",
      "Decoding: it was misunderstanding .\n",
      "Target:   it was a misunderstanding .\n",
      "\n",
      "Decoding: how concerned the concerned should i be ?\n",
      "Target:   how concerned should i be ?\n",
      "\n",
      "Decoding: that you you involved conrad ... is unforgivable . i am now your enemy .\n",
      "Target:   that you 've involved conrad ... is unforgivable . i am now your enemy .\n",
      "\n",
      "Decoding: hi . i todd aubochon . .\n",
      "Target:   hi . i 'm todd aubochon .\n",
      "\n",
      "Decoding: modelling small-group dynamics in formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "Target:   modelling small-group dynamics in the formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "\n",
      "Decoding: it 's somebody idea of a joke .\n",
      "Target:   it 's somebody 's idea of a joke .\n",
      "\n",
      "Decoding: come on . these are typically sentimental gestures of depraved industrialist .\n",
      "Target:   come on . these are the typically sentimental gestures of a depraved industrialist .\n",
      "\n",
      "Decoding: children . children hugh crain built the house for . the children he never had .\n",
      "Target:   the children . the children hugh crain built the house for . the children he never had .\n",
      "\n",
      "Decoding: who watts ? ?\n",
      "Target:   who 's watts ?\n",
      "\n",
      "Decoding: is that a question of compassion or science ?\n",
      "Target:   is that the question of compassion or science ?\n",
      "\n",
      "Decoding: there 's carriage house house around back .\n",
      "Target:   there 's a carriage house around back .\n",
      "\n",
      "Decoding: and those are luke and mine .\n",
      "Target:   and those are luke 's and mine .\n",
      "\n",
      "Decoding: so why did you need the addam family mansion for a scientific test ?\n",
      "Target:   so why did you need the addam 's family mansion for a scientific test ?\n",
      "\n",
      "Decoding: -- how much is this car worth worth ?\n",
      "Target:   -- how much is this car worth ?\n",
      "\n",
      "Decoding: so could you ! is this some fucked up idea idea of art , putting someone else 's name to painting ?\n",
      "Target:   so could you ! is this some fucked up idea of art , putting someone else 's name to a painting ?\n",
      "\n",
      "Decoding: and why did n't marrow tell < u > us < /u > ? does n't he trust trust women ? that fuck .\n",
      "Target:   and why did n't marrow tell < u > us < /u > ? does n't he trust women ? that fuck .\n",
      "\n",
      "Decoding: nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than nightmare . --\n",
      "Target:   nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than a nightmare . --\n",
      "\n",
      "Decoding: why are you working with her ? mary lambretta was thrown out of department for trying to get a ph .d . in psychic studies .\n",
      "Target:   why are you working with her ? mary lambretta was thrown out of the department for trying to get a ph .d . in psychic studies .\n",
      "\n",
      "Decoding: but if the group knows it 's being studied as a group , you contaminate the results . deception is minor .\n",
      "Target:   but if the group knows it 's being studied as a group , you contaminate the results . the deception is minor .\n",
      "\n",
      "Decoding: not the way you 've constructed your group , it just not ethical !\n",
      "Target:   not the way you 've constructed your group , it 's just not ethical !\n",
      "\n",
      "Decoding: it 's still electric shock !\n",
      "Target:   it 's still an electric shock !\n",
      "\n",
      "Decoding: nothing , if you leave right now . there 's a war going on all around us . do n't get in way way , please .\n",
      "Target:   nothing , if you leave right now . there 's a war going on all around us . do n't get in the way , please .\n",
      "\n",
      "Decoding: what happen to us , nell ?\n",
      "Target:   what 'll happen to us , nell ?\n",
      "\n",
      "Decoding: i was n't thinking about my mother bathroom .\n",
      "Target:   i was n't thinking about my mother 's bathroom .\n",
      "\n",
      "Decoding: so ... smell ... is ... smell is the sense that triggers the most powerful most memories . and a memory can trigger a smell .\n",
      "Target:   so ... smell ... is ... smell is the sense that triggers the most powerful memories . and a memory can trigger a smell .\n",
      "\n",
      "Decoding: in bathroom bathroom in my mother 's room , toilet was next to an old wooden table . it smelled like that wood .\n",
      "Target:   in the bathroom in my mother 's room , the toilet was next to an old wooden table . it smelled like that wood .\n",
      "\n",
      "Decoding: for american you do a good imitation of british at there most most apologetic . pardon me . excuse me , sorry , sorry ...\n",
      "Target:   for an american you do a good imitation of the british at their most apologetic . pardon me . excuse me , sorry , sorry ...\n",
      "\n",
      "Decoding: but there 's sad catch to the story .\n",
      "Target:   but there 's a sad catch to the story .\n",
      "\n",
      "Decoding: nell . good enough . and i jim .\n",
      "Target:   nell . good enough . and i 'm jim .\n",
      "\n",
      "Decoding: there were no children . rene died , and < u > than < /u > hugh crain built all of this , and then he died . his heart was broken .\n",
      "Target:   there were no children . rene died , and < u > then < /u > hugh crain built all of this , and then he died . his heart was broken .\n",
      "\n",
      "Decoding: that ? that 's a hill house .\n",
      "Target:   that ? that 's hill house .\n",
      "\n",
      "Decoding: here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on insomnia charts .\n",
      "Target:   here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on the insomnia charts .\n",
      "\n",
      "Decoding: no . if he lost somewhere somewhere in the house ... he 'll have to stay lost until tomorrow , until the night is over . what we have to do now is be together , with nell .\n",
      "Target:   no . if he 's lost somewhere in the house ... he 'll have to stay lost until tomorrow , until the night is over . what we have to do now is be together , with nell .\n",
      "\n",
      "Decoding: yes . but i promised i would n't let her alone whole night night .\n",
      "Target:   yes . but i promised i would n't let her alone the whole night .\n",
      "\n",
      "Decoding: get blanket !\n",
      "Target:   get a blanket !\n",
      "\n",
      "Decoding: rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she got got it .\n",
      "Target:   rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she got it .\n",
      "\n",
      "Decoding: that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of the fence just little little more comfortable . why would that be ?\n",
      "Target:   that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of the fence just a little more comfortable . why would that be ?\n",
      "\n",
      "Decoding: i 'm with dr . marrow group . i 'm supposed to check in with mrs . dudley up at the house . is she here ?\n",
      "Target:   i 'm with dr . marrow 's group . i 'm supposed to check in with mrs . dudley up at the house . is she here ?\n",
      "\n",
      "Decoding: yeah , i 'm mister dudley , caretaker . what are you doing here ?\n",
      "Target:   yeah , i 'm mister dudley , the caretaker . what are you doing here ?\n",
      "\n",
      "Decoding: well , i never lived with beauty beauty . you must love working here .\n",
      "Target:   well , i 've never lived with beauty . you must love working here .\n",
      "\n",
      "Decoding: it make soup or answer the door . ca n't do both .\n",
      "Target:   it 's make the soup or answer the door . ca n't do both .\n",
      "\n",
      "Decoding: nell , it makes sense . it all all makes sense . you and i , we were scaring each other , working each other up .\n",
      "Target:   nell , it makes sense . it all makes sense . you and i , we were scaring each other , working each other up .\n",
      "\n",
      "Decoding: this is real , i 'm not making it up ! theo , you saw it ! you were there -- banging and last night . you , you all saw painting !\n",
      "Target:   this is real , i 'm not making it up ! theo , you saw it ! you were there -- the banging and last night . you , you all saw the painting !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for decoding, target in errors:\n",
    "    print(\"Decoding: \" + \" \".join(decoding))\n",
    "    print(\"Target:   \" + \" \".join(target) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
