{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import nltk\n",
    "\n",
    "from correct_text import train, decode, create_model, DefaultPTBConfig, DefaultMovieDialogConfig\n",
    "from text_correcter_data_readers import PTBDataReader, MovieDialogReader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_data_path = \"/Users/atpaino/data/textcorrecter/dialog_corpus\"\n",
    "train_path = os.path.join(root_data_path, \"cleaned_dialog_train.txt\")#\"ptb.train.txt\") \n",
    "val_path = os.path.join(root_data_path, \"cleaned_dialog_val.txt\")#\"ptb.valid.txt\")\n",
    "test_path = os.path.join(root_data_path, \"cleaned_dialog_test.txt\")\n",
    "model_path = os.path.join(root_data_path, \"dialog_correcter_model\")\n",
    "config = DefaultMovieDialogConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(data_reader, train_path, val_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "model = create_model(sess, True, model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path, dropout_prob=0.9, replacement_prob=0.9, dataset_copies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dec(s, verbose=True):\n",
    "    return decode(sess, model, data_reader, [s], verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test a sample from the test dataset.\n",
    "decoded = decode(sess, model, data_reader,\n",
    "                 [\"you have girlfriend\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout and replacement rates of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build corpus and decode hypotheses.\n",
    "baseline_hypotheses = defaultdict(list)  # The model's input\n",
    "model_hypotheses = defaultdict(list)  # The actual model's predictions\n",
    "targets = defaultdict(list)\n",
    "blah = 0\n",
    "\n",
    "for source, target in data_reader.read_token_samples(test_path):\n",
    "    \n",
    "    matching_buckets = [i for i, bucket in enumerate(model.buckets) if len(source) < bucket[0]]\n",
    "    if not matching_buckets:\n",
    "        continue\n",
    "    bucket_id = matching_buckets[0]\n",
    "    \n",
    "    model_hypotheses[bucket_id].append(dec(\" \".join(source), verbose=False)[0])\n",
    "    \n",
    "    # Replace out of vocab words with \"UNK\" in the baseline hypothesis to make it a little fairer.\n",
    "    baseline_hypothesis = [word if word in data_reader.token_to_id else MovieDialogReader.UNKNOWN_TOKEN\n",
    "                           for word in source]\n",
    "    baseline_hypotheses[bucket_id].append(baseline_hypothesis)\n",
    "    \n",
    "    # nltk.corpus_bleu expects a list of one or more reference tranlsations per sample,\n",
    "    # so we wrap the target list in another list here.\n",
    "    targets[bucket_id].append([target])\n",
    "    \n",
    "#     blah += 1\n",
    "#     if blah > 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for bucket_id in targets.keys():\n",
    "    baseline_bleu_score = nltk.translate.bleu_score.corpus_bleu(targets[bucket_id], baseline_hypotheses[bucket_id])\n",
    "    model_bleu_score = nltk.translate.bleu_score.corpus_bleu(targets[bucket_id], model_hypotheses[bucket_id])\n",
    "    print(\"Bucket {}: {}\".format(bucket_id, model.buckets[bucket_id]))\n",
    "    print(\"\\tBaseline BLEU = {}\\n\\tModel BLEU = {}\".format(baseline_bleu_score, model_bleu_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
