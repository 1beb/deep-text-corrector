{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import nltk\n",
    "\n",
    "from correct_text import train, decode, decode_sentence, evaluate_accuracy, create_model,\\\n",
    "    get_corrective_tokens, DefaultPTBConfig, DefaultMovieDialogConfig\n",
    "from text_correcter_data_readers import PTBDataReader, MovieDialogReader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_data_path = \"/Users/atpaino/data/textcorrecter/dialog_corpus\"\n",
    "train_path = os.path.join(root_data_path, \"cleaned_dialog_train.txt\")\n",
    "val_path = os.path.join(root_data_path, \"cleaned_dialog_val.txt\")\n",
    "test_path = os.path.join(root_data_path, \"cleaned_dialog_test.txt\")\n",
    "model_path = os.path.join(root_data_path, \"dialog_correcter_model\")\n",
    "config = DefaultMovieDialogConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data; train = /Users/atpaino/data/textcorrecter/dialog_corpus/cleaned_dialog_train.txt, test = /Users/atpaino/data/textcorrecter/dialog_corpus/cleaned_dialog_val.txt\n",
      "Creating 2 layers of 512 units.\n",
      "Reading model parameters from /Users/atpaino/data/textcorrecter/dialog_corpus/dialog_correcter_model/translate.ckpt-15000\n",
      "Training bucket sizes: [226666, 98064, 56724, 80504]\n",
      "Total train size: 461958.0\n",
      "global step 15100 learning rate 0.4049 step-time 4.43 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.12\n",
      "  eval: bucket 3 perplexity 1.67\n",
      "global step 15200 learning rate 0.4049 step-time 5.28 perplexity 1.12\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.13\n",
      "  eval: bucket 2 perplexity 1.15\n",
      "  eval: bucket 3 perplexity 1.16\n",
      "global step 15300 learning rate 0.4049 step-time 4.65 perplexity 1.06\n",
      "  eval: bucket 0 perplexity 1.07\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 15400 learning rate 0.4049 step-time 4.84 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.08\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 15500 learning rate 0.4049 step-time 5.04 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.11\n",
      "global step 15600 learning rate 0.4049 step-time 5.26 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.10\n",
      "global step 15700 learning rate 0.4049 step-time 3.95 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 15800 learning rate 0.4049 step-time 4.10 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 15900 learning rate 0.4049 step-time 4.89 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.10\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 16000 learning rate 0.4049 step-time 3.45 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 16100 learning rate 0.4049 step-time 4.40 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 16200 learning rate 0.4008 step-time 3.99 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 16300 learning rate 0.4008 step-time 4.17 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 16400 learning rate 0.4008 step-time 4.15 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 16500 learning rate 0.3968 step-time 4.34 perplexity 1.24\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.17\n",
      "  eval: bucket 2 perplexity 1.27\n",
      "  eval: bucket 3 perplexity 3.94\n",
      "global step 16600 learning rate 0.3928 step-time 4.46 perplexity 1.11\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.13\n",
      "global step 16700 learning rate 0.3928 step-time 3.96 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.10\n",
      "global step 16800 learning rate 0.3928 step-time 4.99 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.09\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 16900 learning rate 0.3928 step-time 5.05 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 17000 learning rate 0.3928 step-time 4.13 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.09\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 17100 learning rate 0.3928 step-time 4.18 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 17200 learning rate 0.3928 step-time 4.32 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 17300 learning rate 0.3928 step-time 4.70 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.12\n",
      "  eval: bucket 3 perplexity 1.90\n",
      "global step 17400 learning rate 0.3889 step-time 4.17 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.10\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 17500 learning rate 0.3850 step-time 4.86 perplexity 1.05\n",
      "  eval: bucket 0 perplexity 1.07\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 17600 learning rate 0.3850 step-time 4.35 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 17700 learning rate 0.3850 step-time 4.67 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 17800 learning rate 0.3850 step-time 4.88 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.08\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 17900 learning rate 0.3850 step-time 3.76 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 18000 learning rate 0.3850 step-time 4.77 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 18100 learning rate 0.3850 step-time 4.17 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 18200 learning rate 0.3850 step-time 4.38 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 18300 learning rate 0.3850 step-time 4.79 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 18400 learning rate 0.3850 step-time 4.57 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 18500 learning rate 0.3812 step-time 4.42 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 18600 learning rate 0.3812 step-time 4.36 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.29\n",
      "global step 18700 learning rate 0.3812 step-time 4.30 perplexity 1.06\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.10\n",
      "global step 18800 learning rate 0.3774 step-time 4.54 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.11\n",
      "global step 18900 learning rate 0.3774 step-time 4.43 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19000 learning rate 0.3774 step-time 4.48 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19100 learning rate 0.3774 step-time 4.63 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19200 learning rate 0.3774 step-time 4.42 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19300 learning rate 0.3774 step-time 4.86 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 19400 learning rate 0.3774 step-time 4.21 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 19500 learning rate 0.3774 step-time 4.68 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19600 learning rate 0.3736 step-time 4.53 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 19700 learning rate 0.3736 step-time 4.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19800 learning rate 0.3736 step-time 4.47 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 19900 learning rate 0.3736 step-time 4.47 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 20000 learning rate 0.3699 step-time 4.98 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 20100 learning rate 0.3662 step-time 4.56 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 20200 learning rate 0.3662 step-time 4.34 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 20300 learning rate 0.3662 step-time 4.53 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 20400 learning rate 0.3662 step-time 4.14 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 20500 learning rate 0.3662 step-time 4.47 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 20600 learning rate 0.3662 step-time 4.70 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 20700 learning rate 0.3625 step-time 4.23 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 20800 learning rate 0.3625 step-time 4.91 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 20900 learning rate 0.3625 step-time 4.00 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21000 learning rate 0.3625 step-time 4.09 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21100 learning rate 0.3625 step-time 4.33 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21200 learning rate 0.3625 step-time 4.55 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 21300 learning rate 0.3589 step-time 3.99 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.09\n",
      "global step 21400 learning rate 0.3589 step-time 3.88 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 21500 learning rate 0.3589 step-time 4.92 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21600 learning rate 0.3553 step-time 4.94 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 21700 learning rate 0.3517 step-time 4.54 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 21800 learning rate 0.3517 step-time 4.11 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 21900 learning rate 0.3517 step-time 4.41 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22000 learning rate 0.3517 step-time 4.89 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22100 learning rate 0.3482 step-time 4.86 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22200 learning rate 0.3482 step-time 4.65 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22300 learning rate 0.3482 step-time 4.88 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 22400 learning rate 0.3482 step-time 4.05 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 22500 learning rate 0.3447 step-time 4.91 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.00\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 22600 learning rate 0.3413 step-time 4.51 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 22700 learning rate 0.3413 step-time 4.40 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 22800 learning rate 0.3413 step-time 5.01 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 22900 learning rate 0.3413 step-time 4.30 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23000 learning rate 0.3413 step-time 5.05 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 23100 learning rate 0.3379 step-time 4.77 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23200 learning rate 0.3345 step-time 4.96 perplexity 1.04\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23300 learning rate 0.3311 step-time 4.40 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 23400 learning rate 0.3311 step-time 4.99 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 23500 learning rate 0.3311 step-time 4.76 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 23600 learning rate 0.3311 step-time 4.01 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.00\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 23700 learning rate 0.3311 step-time 5.20 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 23800 learning rate 0.3278 step-time 4.94 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 23900 learning rate 0.3278 step-time 4.35 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 24000 learning rate 0.3278 step-time 4.19 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.08\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 24100 learning rate 0.3278 step-time 4.22 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 24200 learning rate 0.3278 step-time 4.77 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 24300 learning rate 0.3278 step-time 4.72 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 24400 learning rate 0.3278 step-time 4.39 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 24500 learning rate 0.3246 step-time 4.21 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 24600 learning rate 0.3246 step-time 4.75 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 24700 learning rate 0.3213 step-time 4.60 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 24800 learning rate 0.3213 step-time 4.87 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 24900 learning rate 0.3213 step-time 4.09 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.08\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.03\n",
      "global step 25000 learning rate 0.3213 step-time 4.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25100 learning rate 0.3213 step-time 4.76 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 25200 learning rate 0.3181 step-time 4.70 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 25300 learning rate 0.3181 step-time 3.98 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.09\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 25400 learning rate 0.3181 step-time 4.81 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25500 learning rate 0.3149 step-time 4.44 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25600 learning rate 0.3149 step-time 4.19 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 25700 learning rate 0.3149 step-time 4.30 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 25800 learning rate 0.3149 step-time 4.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.00\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 25900 learning rate 0.3149 step-time 4.67 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26000 learning rate 0.3149 step-time 4.30 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.03\n",
      "global step 26100 learning rate 0.3149 step-time 4.62 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26200 learning rate 0.3149 step-time 4.79 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 26300 learning rate 0.3118 step-time 5.10 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26400 learning rate 0.3086 step-time 5.24 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.06\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 26500 learning rate 0.3056 step-time 4.36 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 26600 learning rate 0.3056 step-time 5.01 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 26700 learning rate 0.3056 step-time 4.78 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 26800 learning rate 0.3056 step-time 4.74 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 26900 learning rate 0.3056 step-time 4.01 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 27000 learning rate 0.3056 step-time 5.04 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 27100 learning rate 0.3025 step-time 4.89 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 27200 learning rate 0.3025 step-time 5.43 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 27300 learning rate 0.2995 step-time 5.54 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 27400 learning rate 0.2995 step-time 4.87 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 27500 learning rate 0.2965 step-time 4.63 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 27600 learning rate 0.2965 step-time 4.44 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 27700 learning rate 0.2965 step-time 4.45 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.06\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 27800 learning rate 0.2965 step-time 4.09 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 27900 learning rate 0.2965 step-time 3.88 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28000 learning rate 0.2965 step-time 4.56 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28100 learning rate 0.2965 step-time 3.83 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.11\n",
      "global step 28200 learning rate 0.2965 step-time 4.82 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 28300 learning rate 0.2935 step-time 4.37 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.08\n",
      "global step 28400 learning rate 0.2935 step-time 4.55 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 28500 learning rate 0.2906 step-time 3.96 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.07\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28600 learning rate 0.2906 step-time 4.37 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 28700 learning rate 0.2906 step-time 3.33 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 28800 learning rate 0.2906 step-time 4.32 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 28900 learning rate 0.2877 step-time 5.05 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 29000 learning rate 0.2848 step-time 4.44 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.07\n",
      "  eval: bucket 2 perplexity 1.02\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29100 learning rate 0.2820 step-time 4.17 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.08\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29200 learning rate 0.2820 step-time 5.45 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29300 learning rate 0.2791 step-time 4.52 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29400 learning rate 0.2791 step-time 4.38 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 29500 learning rate 0.2791 step-time 4.19 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 29600 learning rate 0.2791 step-time 3.82 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 29700 learning rate 0.2791 step-time 4.26 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 29800 learning rate 0.2791 step-time 4.11 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.05\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 29900 learning rate 0.2763 step-time 4.76 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30000 learning rate 0.2736 step-time 4.08 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30100 learning rate 0.2736 step-time 4.48 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30200 learning rate 0.2708 step-time 4.07 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.05\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30300 learning rate 0.2708 step-time 5.29 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30400 learning rate 0.2708 step-time 4.75 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.03\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 30500 learning rate 0.2708 step-time 5.28 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.05\n",
      "global step 30600 learning rate 0.2681 step-time 5.95 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.04\n",
      "global step 30700 learning rate 0.2655 step-time 4.22 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.03\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30800 learning rate 0.2655 step-time 5.12 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.04\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 30900 learning rate 0.2655 step-time 4.57 perplexity 1.03\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 31000 learning rate 0.2655 step-time 4.46 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.01\n",
      "  eval: bucket 1 perplexity 1.02\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 31100 learning rate 0.2655 step-time 4.41 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.01\n",
      "  eval: bucket 2 perplexity 1.06\n",
      "  eval: bucket 3 perplexity 1.07\n",
      "global step 31200 learning rate 0.2655 step-time 4.72 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.04\n",
      "  eval: bucket 2 perplexity 1.04\n",
      "  eval: bucket 3 perplexity 1.06\n",
      "global step 31300 learning rate 0.2655 step-time 3.63 perplexity 1.02\n",
      "  eval: bucket 0 perplexity 1.02\n",
      "  eval: bucket 1 perplexity 1.03\n",
      "  eval: bucket 2 perplexity 1.05\n",
      "  eval: bucket 3 perplexity 1.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c4f4a34e9f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/atpaino/github/deep-text-correcter/correct_text.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_reader, train_path, test_path, model_path)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 train_data, bucket_id)\n\u001b[1;32m    181\u001b[0m             _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n\u001b[0;32m--> 182\u001b[0;31m                                          target_weights, bucket_id, False)\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mstep_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atpaino/github/deep-text-correcter/text_correcter_models.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(data_reader, train_path, val_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path, dropout_prob=0.25, replacement_prob=0.25, dataset_copies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrective_tokens = get_corrective_tokens(data_reader, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from /Users/atpaino/data/textcorrecter/dialog_corpus/dialog_correcter_model/translate.ckpt-31300\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "model = create_model(sess, True, model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: you must have girlfriend\n",
      "Output: you must have a girlfriend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test a sample from the test dataset.\n",
    "decoded = decode_sentence(sess, model, data_reader, \"you must have girlfriend\", corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "Output: did n't you say that they 're going to develop this revolutionary a new thing ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded = decode_sentence(sess, model, data_reader,\n",
    "                          \"did n't you say that they 're going to develop this revolutionary new thing ...\",\n",
    "                          corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kvothe', 'went', 'to', 'the', 'market']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"kvothe went to market\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blablahblah', 'and', 'bladdddd', 'went', 'to', 'the', 'market']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"blablahblah and bladdddd went to market\", corrective_tokens=corrective_tokens,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'you', 'have', 'a', 'book']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"do you have book\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'did', 'better', 'than', 'him']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"she did better then him\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8282\n",
      "\tModel BLEU = 0.8444\n",
      "\tBaseline Accuracy: 0.9041\n",
      "\tModel Accuracy: 0.9355\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8794\n",
      "\tModel BLEU = 0.8953\n",
      "\tBaseline Accuracy: 0.7711\n",
      "\tModel Accuracy: 0.8434\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8989\n",
      "\tModel BLEU = 0.8884\n",
      "\tBaseline Accuracy: 0.7619\n",
      "\tModel Accuracy: 0.7810\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.8977\n",
      "\tModel BLEU = 0.9109\n",
      "\tBaseline Accuracy: 0.6000\n",
      "\tModel Accuracy: 0.6480\n"
     ]
    }
   ],
   "source": [
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: you beg for mercy in a second .\n",
      "Target:   you 'll beg for mercy in a second .\n",
      "\n",
      "Decoding: i 'm dying for a shower . you could use the one too . and we 'd better check that bandage .\n",
      "Target:   i 'm dying for a shower . you could use one too . and we 'd better check that bandage .\n",
      "\n",
      "Decoding: whatever ... they 've become hotshot computer guys so they get a job to build el computer grande ... skynet ... for the government . right ?\n",
      "Target:   whatever ... they become the hotshot computer guys so they get the job to build el computer grande ... skynet ... for the government . right ?\n",
      "\n",
      "Decoding: did n't you say that they 're going to develop this revolutionary a new thing ...\n",
      "Target:   did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "\n",
      "Decoding: bag some z ?\n",
      "Target:   bag some z 's ?\n",
      "\n",
      "Decoding: sleep . it 'll be a light soon .\n",
      "Target:   sleep . it 'll be light soon .\n",
      "\n",
      "Decoding: well , at least i know what to name him . i do n't suppose you 'd know who father is ? so i do n't tell him to get lost when i meet him .\n",
      "Target:   well , at least i know what to name him . i do n't suppose you 'd know who the father is ? so i do n't tell him to get lost when i meet him .\n",
      "\n",
      "Decoding: we got ta get you to doctor .\n",
      "Target:   we got ta get you to a doctor .\n",
      "\n",
      "Decoding: hunter killers . patrol machines . a build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "Target:   hunter killers . patrol machines . build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "\n",
      "Decoding: but outside , it 's a living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "Target:   but outside , it 's living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "\n",
      "Decoding: you heard enough . decide . are you going to release me ?\n",
      "Target:   you 've heard enough . decide . are you going to release me ?\n",
      "\n",
      "Decoding: okay . okay . but this ... cyborg ... if it metal ...\n",
      "Target:   okay . okay . but this ... cyborg ... if it 's metal ...\n",
      "\n",
      "Decoding: you go naked . something about the field generated by living organism . nothing dead will go .\n",
      "Target:   you go naked . something about the field generated by a living organism . nothing dead will go .\n",
      "\n",
      "Decoding: ca n't . nobody goes home . nobody else comes through . it just him and me .\n",
      "Target:   ca n't . nobody goes home . nobody else comes through . it 's just him and me .\n",
      "\n",
      "Decoding: i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? sort of retroactive abortion ?\n",
      "Target:   i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? a sort of retroactive abortion ?\n",
      "\n",
      "Decoding: skynet . a computer defense system built for sac-norad by cyber dynamics . modified series 4800 .\n",
      "Target:   skynet . a computer defense system built for sac-norad by cyber dynamics . a modified series 4800 .\n",
      "\n",
      "Decoding: a year 2027 ?\n",
      "Target:   the year 2027 ?\n",
      "\n",
      "Decoding: with one thirty a second under perry , from '21 to '27 --\n",
      "Target:   with the one thirty second under perry , from '21 to '27 --\n",
      "\n",
      "Decoding: why do n't you just stretch out here and get some sleep . it take your mom 's a good hour to get here from redlands .\n",
      "Target:   why do n't you just stretch out here and get some sleep . it 'll take your mom a good hour to get here from redlands .\n",
      "\n",
      "Decoding: lieutenant , are you sure it them ? maybe i should see the ... bodies .\n",
      "Target:   lieutenant , are you sure it 's them ? maybe i should see the ... bodies .\n",
      "\n",
      "Decoding: i already did . no answer at the door and the apartment manager 's out . i keeping them there .\n",
      "Target:   i already did . no answer at the door and the apartment manager 's out . i 'm keeping them there .\n",
      "\n",
      "Decoding: that stuff two hours cold .\n",
      "Target:   that stuff 's two hours cold .\n",
      "\n",
      "Decoding: you got ta be kidding me . the new guys 'll be short-stroking it over this one . one-day pattern killer .\n",
      "Target:   you got ta be kidding me . the new guys 'll be short-stroking it over this one . a one-day pattern killer .\n",
      "\n",
      "Decoding: give me a short version .\n",
      "Target:   give me the short version .\n",
      "\n",
      "Decoding: because it 's fair . give me the next quarter . if you still feel this way , vote your shares ...\n",
      "Target:   because it 's fair . give me next quarter . if you still feel this way , vote your shares ...\n",
      "\n",
      "Decoding: it 's probably will . in fact , i 'd go so far as to say it 's almost certainly will , in time . why should i settle for that ?\n",
      "Target:   it probably will . in fact , i 'd go so far as to say it almost certainly will , in time . why should i settle for that ?\n",
      "\n",
      "Decoding: stock will turn .\n",
      "Target:   the stock will turn .\n",
      "\n",
      "Decoding: you want to know what it is ? what 's it all about ? john . chapter nine . verse twenty-five .\n",
      "Target:   you want to know what it is ? what it 's all about ? john . chapter nine . verse twenty-five .\n",
      "\n",
      "Decoding: i only mention it because i took a test this afternoon , down on montgomery street .\n",
      "Target:   i only mention it because i took the test this afternoon , down on montgomery street .\n",
      "\n",
      "Decoding: christine ! mister van orton is valued customer ...\n",
      "Target:   christine ! mister van orton is a valued customer ...\n",
      "\n",
      "Decoding: a single ?\n",
      "Target:   single ?\n",
      "\n",
      "Decoding: there 's another gig starting in saudi arabia . i just a walk-on this time though . bit-part .\n",
      "Target:   there 's another gig starting in saudi arabia . i 'm just a walk-on this time though . bit-part .\n",
      "\n",
      "Decoding: no ! you take another step , i shoot ! they 're trying to kill me ...\n",
      "Target:   no ! you take another step , i 'll shoot ! they 're trying to kill me ...\n",
      "\n",
      "Decoding: listen very carefully , i 'm telling the truth ... this is a game . this was all the game .\n",
      "Target:   listen very carefully , i 'm telling the truth ... this is the game . this was all the game .\n",
      "\n",
      "Decoding: that 's gun . that 's ... that 's not automatic . the guard had an automatic ...\n",
      "Target:   that gun . that ... that 's not automatic . the guard had an automatic ...\n",
      "\n",
      "Decoding: take a picture out .\n",
      "Target:   take the picture out .\n",
      "\n",
      "Decoding: yeah . first communion . are n't i little angel ?\n",
      "Target:   yeah . first communion . are n't i a little angel ?\n",
      "\n",
      "Decoding: let me go get some clothes on . we talk , okay ? be right back .\n",
      "Target:   let me go get some clothes on . we 'll talk , okay ? be right back .\n",
      "\n",
      "Decoding: i 'm tired . i 'm sorry , i should go . i 've been enough of nuisance .\n",
      "Target:   i 'm tired . i 'm sorry , i should go . i 've been enough of a nuisance .\n",
      "\n",
      "Decoding: they said five hundred . i said six . they said man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "Target:   they said five hundred . i said six . they said the man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "\n",
      "Decoding: i have a confession to make . someone gave me six-hundred dollars to spill a drinks on you , as a practical joke .\n",
      "Target:   i have a confession to make . someone gave me six-hundred dollars to spill drinks on you , as a practical joke .\n",
      "\n",
      "Decoding: maitre d ' called you christine .\n",
      "Target:   the maitre d ' called you christine .\n",
      "\n",
      "Decoding: i know owner of campton place . i could talk to him in the morning .\n",
      "Target:   i know the owner of campton place . i could talk to him in the morning .\n",
      "\n",
      "Decoding: fresh shirt ...\n",
      "Target:   a fresh shirt ...\n",
      "\n",
      "Decoding: investment banking . moving money from a place to place .\n",
      "Target:   investment banking . moving money from place to place .\n",
      "\n",
      "Decoding: what 's the c .r .s . ?\n",
      "Target:   what 's c .r .s . ?\n",
      "\n",
      "Decoding: this is a c .r .s .\n",
      "Target:   this is c .r .s .\n",
      "\n",
      "Decoding: their ladder here .\n",
      "Target:   there 's a ladder here .\n",
      "\n",
      "Decoding: this is n't attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "Target:   this is n't an attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "\n",
      "Decoding: are you suggesting we wait till someone 's finds us ?\n",
      "Target:   are you suggesting we wait till someone finds us ?\n",
      "\n",
      "Decoding: `` ... wait for help . '' wait for help . i 'm not opening that specifically warns me not to .\n",
      "Target:   `` ... wait for help . '' wait for help . i 'm not opening a door that specifically warns me not to .\n",
      "\n",
      "Decoding: read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use the emergency ... ``\n",
      "Target:   read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use emergency ... ``\n",
      "\n",
      "Decoding: long story . i found this key in the mouth of wooden harlequin .\n",
      "Target:   long story . i found this key in the mouth of a wooden harlequin .\n",
      "\n",
      "Decoding: how do you know that way ?\n",
      "Target:   how do you know that 's the way ?\n",
      "\n",
      "Decoding: it 's run by company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "Target:   it 's run by a company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "\n",
      "Decoding: you got to be kidding .\n",
      "Target:   you 've got to be kidding .\n",
      "\n",
      "Decoding: i do n't think he breathing .\n",
      "Target:   i do n't think he 's breathing .\n",
      "\n",
      "Decoding: a bad month . you did exact the same thing to me last week .\n",
      "Target:   a bad month . you did the exact same thing to me last week .\n",
      "\n",
      "Decoding: yeah , yeah . she 's called a cab . said something about catching plane .\n",
      "Target:   yeah , yeah . she called a cab . said something about catching a plane .\n",
      "\n",
      "Decoding: oh , god yes please . thanks , man . i take you up on that .\n",
      "Target:   oh , god yes please . thanks , man . i 'll take you up on that .\n",
      "\n",
      "Decoding: this ... ? oh , this is just ... this is bill .\n",
      "Target:   this ... ? oh , this is just ... this is the bill .\n",
      "\n",
      "Decoding: baby , they were all over the house with metal detectors . they switched your gun with look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "Target:   baby , they were all over the house with metal detectors . they switched your gun with a look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "\n",
      "Decoding: you dodged bullet .\n",
      "Target:   you dodged a bullet .\n",
      "\n",
      "Decoding: c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i 've almost got you into .\n",
      "Target:   c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i almost got you into .\n",
      "\n",
      "Decoding: it 's profound life experience .\n",
      "Target:   it 's a profound life experience .\n",
      "\n",
      "Decoding: you 've heard of it . you 've seen other people having it . they 're entertainment service , but more than that .\n",
      "Target:   you 've heard of it . you 've seen other people having it . they 're an entertainment service , but more than that .\n",
      "\n",
      "Decoding: they make your life fun . there 's only guarantee is you will not be bored .\n",
      "Target:   they make your life fun . their only guarantee is you will not be bored .\n",
      "\n",
      "Decoding: not after i done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "Target:   not after i 'm done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "\n",
      "Decoding: that 's why it 's a classic . come on , man ... how 'bout hug ... ?\n",
      "Target:   that 's why it 's a classic . come on , man ... how 'bout a hug ... ?\n",
      "\n",
      "Decoding: how much is it ? a few thousand , at least . a rolex like that ... lucky for you 've missed it .\n",
      "Target:   how much is it ? a few thousand , at least . a rolex like that ... lucky for you they missed it .\n",
      "\n",
      "Decoding: i told you , they hired me over the phone . i 've never met anyone .\n",
      "Target:   i told you , they hired me over the phone . i never met anyone .\n",
      "\n",
      "Decoding: i do n't want money . i 'm pulling back curtain . i 'm here to meet the wizard .\n",
      "Target:   i do n't want money . i 'm pulling back the curtain . i 'm here to meet the wizard .\n",
      "\n",
      "Decoding: tell them the cops are after you ... tell them you got to talk to someone , i 'm threatening to blow the whistle .\n",
      "Target:   tell them the cops are after you ... tell them you 've got to talk to someone , i 'm threatening to blow the whistle .\n",
      "\n",
      "Decoding: they own the whole building . they just move from the floor to floor .\n",
      "Target:   they own the whole building . they just move from floor to floor .\n",
      "\n",
      "Decoding: look , it was just a job . nothing personal , ya know ? i play my part , improvise little . that 's what i 'm good at .\n",
      "Target:   look , it was just a job . nothing personal , ya know ? i play my part , improvise a little . that 's what i 'm good at .\n",
      "\n",
      "Decoding: that 's right -- you 're left-brain the word fetishist .\n",
      "Target:   that 's right -- you 're a left-brain word fetishist .\n",
      "\n",
      "Decoding: one guarantee . payment 's entirely at your brother discretion and , as a gift , dependent on your satisfaction .\n",
      "Target:   one guarantee . payment 's entirely at your brother 's discretion and , as a gift , dependent on your satisfaction .\n",
      "\n",
      "Decoding: your brother was a client with our branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "Target:   your brother was a client with our london branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "\n",
      "Decoding: key ?\n",
      "Target:   the key ?\n",
      "\n",
      "Decoding: nobody 's worried about your father .\n",
      "Target:   nobody worried about your father .\n",
      "\n",
      "Decoding: there 's been a break in . lock this door and stay here . do n't move muscle .\n",
      "Target:   there 's been a break in . lock this door and stay here . do n't move a muscle .\n",
      "\n",
      "Decoding: i do n't know what you 're talking about . what happened ?\n",
      "Target:   i do n't know what you 're talking about . what 's happened ?\n",
      "\n",
      "Decoding: did alarm go off ? the house ... they ... you did n't see ... ?\n",
      "Target:   did the alarm go off ? the house ... they ... you did n't see ... ?\n",
      "\n",
      "Decoding: then then .\n",
      "Target:   goodnight then .\n",
      "\n",
      "Decoding: okay . i think he into some sort of new personal improvement cult .\n",
      "Target:   okay . i think he 's into some sort of new personal improvement cult .\n",
      "\n",
      "Decoding: dinner in the oven .\n",
      "Target:   dinner 's in the oven .\n",
      "\n",
      "Decoding: there was incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "Target:   there was an incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "\n",
      "Decoding: what 's trouble ?\n",
      "Target:   what 's the trouble ?\n",
      "\n",
      "Decoding: mister ... seymour butts .\n",
      "Target:   a mister ... seymour butts .\n",
      "\n",
      "Decoding: what 's the gentleman , maria ?\n",
      "Target:   what gentleman , maria ?\n",
      "\n",
      "Decoding: i would n't mention following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "Target:   i would n't mention the following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "\n",
      "Decoding: i send your regrets . honestly , why must i even bother ?\n",
      "Target:   i 'll send your regrets . honestly , why must i even bother ?\n",
      "\n",
      "Decoding: the hinchberger 's wedding .\n",
      "Target:   the hinchberger wedding .\n",
      "\n",
      "Decoding: invitations : museum gala .\n",
      "Target:   invitations : the museum gala .\n",
      "\n",
      "Decoding: nice touch . does a game use real bullets ... ?\n",
      "Target:   nice touch . does the game use real bullets ... ?\n",
      "\n",
      "Decoding: it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children\n",
      "Target:   it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children .\n",
      "\n",
      "Decoding: find out about a company called the c .r .s . consumer recreation services .\n",
      "Target:   find out about a company called c .r .s . consumer recreation services .\n",
      "\n",
      "Decoding: someone 's playing hardball . it 's complicated . can i ask favor ?\n",
      "Target:   someone 's playing hardball . it 's complicated . can i ask a favor ?\n",
      "\n",
      "Decoding: how 's the concerned should i be ?\n",
      "Target:   how concerned should i be ?\n",
      "\n",
      "Decoding: that you 've a involved conrad ... is unforgivable . i am now your enemy .\n",
      "Target:   that you 've involved conrad ... is unforgivable . i am now your enemy .\n",
      "\n",
      "Decoding: what happened ...\n",
      "Target:   what 's happened ...\n",
      "\n",
      "Decoding: modelling small-group dynamics in formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "Target:   modelling small-group dynamics in the formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "\n",
      "Decoding: come on . these are the typically sentimental gestures of depraved industrialist .\n",
      "Target:   come on . these are the typically sentimental gestures of a depraved industrialist .\n",
      "\n",
      "Decoding: the children . children hugh crain built the house for . the children he never had .\n",
      "Target:   the children . the children hugh crain built the house for . the children he never had .\n",
      "\n",
      "Decoding: obsessive worrier . join club . and you ? i 'd guess ...\n",
      "Target:   obsessive worrier . join the club . and you ? i 'd guess ...\n",
      "\n",
      "Decoding: so why did you need the addam family mansion for a scientific test ?\n",
      "Target:   so why did you need the addam 's family mansion for a scientific test ?\n",
      "\n",
      "Decoding: -- how much is this car 's worth ?\n",
      "Target:   -- how much is this car worth ?\n",
      "\n",
      "Decoding: you do n't really believe it haunted ... do you believe in ghosts ?\n",
      "Target:   you do n't really believe it 's haunted ... do you believe in ghosts ?\n",
      "\n",
      "Decoding: so could you ! is this some fucked up the idea of art , putting someone else 's name to a painting ?\n",
      "Target:   so could you ! is this some fucked up idea of art , putting someone else 's name to a painting ?\n",
      "\n",
      "Decoding: and why did n't marrow tell < u > us < /u > ? does n't he a trust women ? that fuck .\n",
      "Target:   and why did n't marrow tell < u > us < /u > ? does n't he trust women ? that fuck .\n",
      "\n",
      "Decoding: nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than nightmare . --\n",
      "Target:   nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than a nightmare . --\n",
      "\n",
      "Decoding: not the way you 've constructed your group , it just not ethical !\n",
      "Target:   not the way you 've constructed your group , it 's just not ethical !\n",
      "\n",
      "Decoding: children want me . they 're calling me . they need me .\n",
      "Target:   the children want me . they 're calling me . they need me .\n",
      "\n",
      "Decoding: i looked at theo . she had look on her face .\n",
      "Target:   i looked at theo . she had a look on her face .\n",
      "\n",
      "Decoding: i was n't thinking about my mother bathroom .\n",
      "Target:   i was n't thinking about my mother 's bathroom .\n",
      "\n",
      "Decoding: so ... smell ... is ... smell is sense that triggers the most powerful memories . and memory can trigger a smell .\n",
      "Target:   so ... smell ... is ... smell is the sense that triggers the most powerful memories . and a memory can trigger a smell .\n",
      "\n",
      "Decoding: in the bathroom in my mother 's room , toilet was next to old wooden table . it smelled like that wood .\n",
      "Target:   in the bathroom in my mother 's room , the toilet was next to an old wooden table . it smelled like that wood .\n",
      "\n",
      "Decoding: cold sensation . who felt it first ?\n",
      "Target:   the cold sensation . who felt it first ?\n",
      "\n",
      "Decoding: i really ... honored to be part of this study , jim .\n",
      "Target:   i 'm really ... honored to be part of this study , jim .\n",
      "\n",
      "Decoding: nell . good enough . and i jim .\n",
      "Target:   nell . good enough . and i 'm jim .\n",
      "\n",
      "Decoding: that ? that 's a hill house .\n",
      "Target:   that ? that 's hill house .\n",
      "\n",
      "Decoding: here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on insomnia charts .\n",
      "Target:   here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on the insomnia charts .\n",
      "\n",
      "Decoding: you hear the vibrations in the wire . there 's magnetic pulse in the wires , you feel it . i could test it .\n",
      "Target:   you hear the vibrations in the wire . there 's a magnetic pulse in the wires , you feel it . i could test it .\n",
      "\n",
      "Decoding: but experiment was a failure .\n",
      "Target:   but the experiment was a failure .\n",
      "\n",
      "Decoding: he wandering around house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "Target:   he 's wandering around the house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "\n",
      "Decoding: i 'll take her with me to university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "Target:   i 'll take her with me to the university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "\n",
      "Decoding: no , but nell been here longer than i have .\n",
      "Target:   no , but nell 's been here longer than i have .\n",
      "\n",
      "Decoding: rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she 's got it .\n",
      "Target:   rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she got it .\n",
      "\n",
      "Decoding: mrs . dudley be waiting for you .\n",
      "Target:   mrs . dudley 'll be waiting for you .\n",
      "\n",
      "Decoding: that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of fence just a little more comfortable . why would that be ?\n",
      "Target:   that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of the fence just a little more comfortable . why would that be ?\n",
      "\n",
      "Decoding: well , i 've never lived with a beauty . you must love working here .\n",
      "Target:   well , i 've never lived with beauty . you must love working here .\n",
      "\n",
      "Decoding: nell , it makes sense . it 's all makes sense . you and i , we were scaring each other , working each other up .\n",
      "Target:   nell , it makes sense . it all makes sense . you and i , we were scaring each other , working each other up .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for decoding, target in errors:\n",
    "    print(\"Decoding: \" + \" \".join(decoding))\n",
    "    print(\"Target:   \" + \" \".join(target) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
